{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f1f0e436-bb5e-4358-9306-f298005618de",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dfeea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # Deep Learning Model for Cancer vs. Healthy Detection Using Features Extracted from cfDNA Fragment Size \n",
    "\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# This section imports all necessary libraries for reproducibility.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score, roc_auc_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from collections import Counter\n",
    "from sklearn.calibration import calibration_curve\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define output folder for plots and logs\n",
    "output_folder = 'results/temp' # For DELFI based results\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Log file path\n",
    "log_file_path = os.path.join(output_folder, 'prediction_results_all.txt') # Specific log for DELFI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883f2bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ## 2. Model Architecture\n",
    "\n",
    "# This function defines the architecture of the deep neural network. It includes Dense layers, Batch Normalization, ReLU activation, and Dropout for regularization. L2 regularization is also applied to the Dense layers.\n",
    "\n",
    "def build_model(input_dim):\n",
    "    \"\"\"\n",
    "    Builds a sequential Keras model for binary classification.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): The number of input features.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    # First Dense Layer\n",
    "    x = Dense(512, kernel_regularizer=l2(0.013))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    # Second Dense Layer\n",
    "    x = Dense(256, kernel_regularizer=l2(0.013))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    # Third Dense Layer\n",
    "    x = Dense(128, kernel_regularizer=l2(0.013))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    # Output Layer\n",
    "    outputs = Dense(1, activation='sigmoid')(x) # Sigmoid for binary classification\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model with Adam optimizer, binary crossentropy loss, and AUC/accuracy metrics\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['AUC', 'binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f00b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ## 3. Helper Functions for Evaluation and Plotting\n",
    "\n",
    "# This section defines functions to evaluate the model's performance and to plot confusion matrices.\n",
    "\n",
    "def evaluate_and_log(model, X, y, set_name, log_file):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a given dataset and logs the results.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        X (np.array): Features of the dataset.\n",
    "        y (np.array): True labels of the dataset.\n",
    "        set_name (str): Name of the dataset (e.g., \"Train\", \"Validation\", \"Test\").\n",
    "        log_file (file object): File object to write the logs to.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (y_true, y_pred, y_pred_proba, loss, auc, acc, f1_score).\n",
    "    \"\"\"\n",
    "    loss, auc, acc = model.evaluate(X, y, verbose=0)\n",
    "    y_pred_proba = model.predict(X).flatten()\n",
    "    y_pred = np.round(y_pred_proba)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y, y_pred)\n",
    "\n",
    "    print(f\"--- {set_name} Results ---\")\n",
    "    print(f\"Loss: {loss:.4f}, AUC: {auc:.4f}, Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Log results\n",
    "    seed_val = getattr(log_file, 'seed', 'N/A') # Access seed from the file object attributes\n",
    "    log_file.write(f\"{seed_val}\\t{set_name}\\tLoss\\t{loss:.4f}\\n\")\n",
    "    log_file.write(f\"{seed_val}\\t{set_name}\\tAUC\\t{auc:.4f}\\n\")\n",
    "    log_file.write(f\"{seed_val}\\t{set_name}\\tAccuracy\\t{acc:.4f}\\n\")\n",
    "    log_file.write(f\"{seed_val}\\t{set_name}\\tF1_Score\\t{f1:.4f}\\n\")\n",
    "\n",
    "    return y, y_pred, y_pred_proba, loss, auc, acc, f1 # Return metrics as well\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title, filename):\n",
    "    \"\"\"\n",
    "    Plots and saves a confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        labels (list): List of class labels (e.g., ['Negative', 'Positive']).\n",
    "        title (str): Title of the plot.\n",
    "        filename (str): Path to save the plot.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8)) # Create figure and axes explicitly for better control\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, text_kw={'fontsize': 14}, colorbar=True)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=14)\n",
    "    ax.set_ylabel('True Label', fontsize=14)\n",
    "    if len(labels) == 1:\n",
    "        ax.set_xticks([0])\n",
    "        ax.set_xticklabels(labels, fontsize=12)\n",
    "        ax.set_yticks([0])\n",
    "        ax.set_yticklabels(labels, fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75bf3ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ## 4. Data Loading and Preprocessing\n",
    "\n",
    "# This section loads the sample data from `review_data_sample.csv`, separates features and labels, and then prepares lists to store evaluation metrics across multiple runs.\n",
    "\n",
    "# Ensure `review_data_sample.csv` is in the same directory as this notebook.\n",
    "# If not, please provide the correct path to the file.\n",
    "\n",
    "# Load the sample data\n",
    "try:\n",
    "    sample_data = pd.read_csv('review_data_sample.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'review_data_sample.csv' not found. Please ensure the file is in the same directory or provide the correct path.\")\n",
    "    exit() # Exit if the data file is not found\n",
    "\n",
    "# Separate features and labels\n",
    "labels_encoded_filtered_main = sample_data['label'].values\n",
    "main_features_filtered = sample_data.drop('label', axis=1).values\n",
    "\n",
    "print(f\"Loaded sample data with shape: {main_features_filtered.shape}\")\n",
    "print(f\"Class distribution in loaded data: {pd.Series(labels_encoded_filtered_main).value_counts().to_dict()}\")\n",
    "\n",
    "# Initialize lists to store metrics from each run\n",
    "all_val_f1_scores, all_val_auc_scores = [], []\n",
    "all_test_f1_scores, all_test_auc_scores = [], []\n",
    "all_external_cancer_accuracies = [] # This list seems to be for an external dataset not present in this code.\n",
    "\n",
    "# Initialize X_train_scaled for potential later use (e.g., for feature names if needed)\n",
    "X_train_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e902512",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ## 5. Training and Evaluation Loop\n",
    "\n",
    "# This is the main loop where the model is trained and evaluated across 10 different random seeds. For each seed:\n",
    "# 1.  Data is split into training, validation, and test sets.\n",
    "# 2.  SMOTE (Synthetic Minority Over-sampling Technique) is applied to the training data to handle class imbalance.\n",
    "# 3.  Features are scaled using `StandardScaler`.\n",
    "# 4.  The neural network model is built and trained with early stopping.\n",
    "# 5.  Model performance is evaluated on the original training, validation, and test sets.\n",
    "# 6.  Results are logged to a file.\n",
    "\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    # Write header to the log file\n",
    "    log_file.write(\"Seed\\tSet\\tMetric\\tValue\\n\")\n",
    "\n",
    "    # Loop through different random seeds for robust evaluation\n",
    "    for seed in range(10):\n",
    "        print(f\"\\n--- Seed {seed} ---\")\n",
    "        log_file.seed = seed # Attach seed to the file object for logging\n",
    "\n",
    "        # Split data into training, temporary (for validation/test)\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            main_features_filtered, labels_encoded_filtered_main,\n",
    "            test_size=0.3, random_state=seed, stratify=labels_encoded_filtered_main\n",
    "        )\n",
    "        # Split temporary data into test and validation sets\n",
    "        X_test, X_val, y_test, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.33, random_state=seed, stratify=y_temp\n",
    "        )\n",
    "\n",
    "        # Check for empty splits\n",
    "        if X_train.shape[0] == 0 or X_val.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "            print(f\"Warning: Seed {seed} resulted in an empty train/val/test set. Skipping this seed.\")\n",
    "            all_val_f1_scores.append(np.nan)\n",
    "            all_val_auc_scores.append(np.nan)\n",
    "            all_test_f1_scores.append(np.nan)\n",
    "            all_test_auc_scores.append(np.nan)\n",
    "            all_external_cancer_accuracies.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        print(f\"Class distribution before SMOTE (Train): {Counter(y_train)}\")\n",
    "        # Apply SMOTE to handle class imbalance in training data\n",
    "        if len(Counter(y_train)) < 2:\n",
    "            print(f\"Warning: Seed {seed} - Training data has only one class before SMOTE. SMOTE will not be applied.\")\n",
    "            X_train_smote, y_train_smote = X_train, y_train\n",
    "        else:\n",
    "            smote = SMOTE(random_state=seed)\n",
    "            X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"Class distribution after SMOTE (Train): {Counter(y_train_smote)}\")\n",
    "\n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Scale original training data (before SMOTE) for evaluation purposes\n",
    "        X_train_original_scaled = scaler.transform(X_train)\n",
    "        y_train_original = y_train\n",
    "\n",
    "        current_input_dim = X_train_scaled.shape[1]\n",
    "        print(f\"Model input dimension (number of features): {current_input_dim}\")\n",
    "\n",
    "        # Build and compile the model\n",
    "        model = build_model(current_input_dim)\n",
    "        # Set up Early Stopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(X_train_scaled, y_train_smote, epochs=200, batch_size=32,\n",
    "                            validation_data=(X_val_scaled, y_val), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluate and log results for different sets\n",
    "        evaluate_and_log(model, X_train_original_scaled, y_train_original, f\"Train_Original_Seed_{seed}\", log_file)\n",
    "\n",
    "        val_y_true, val_y_pred, _, val_loss, val_auc, val_acc, val_f1 = evaluate_and_log(model, X_val_scaled, y_val, f\"Validation_Seed_{seed}\", log_file)\n",
    "        all_val_f1_scores.append(val_f1)\n",
    "        all_val_auc_scores.append(val_auc)\n",
    "\n",
    "        test_y_true, test_y_pred, _, test_loss, test_auc, test_acc, test_f1 = evaluate_and_log(model, X_test_scaled, y_test, f\"Test_Seed_{seed}\", log_file)\n",
    "        all_test_f1_scores.append(test_f1)\n",
    "        all_test_auc_scores.append(test_auc)\n",
    "\n",
    "        # Plot Confusion Matrix for Test Set (optional, as it was commented out in original logic, but useful)\n",
    "        plot_confusion_matrix(test_y_true, test_y_pred, labels=['0', '1'],\n",
    "                              title=f'Confusion Matrix - Test Set (Seed {seed})',\n",
    "                              filename=os.path.join(output_folder, f'confusion_matrix_test_seed_{seed}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789775ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ## 6. Final Reporting\n",
    "\n",
    "# This section calculates and prints the average F1 scores and AUC scores across all valid seeds for both validation and test sets, providing an overall summary of the model's performance.\n",
    "\n",
    "# Remove NaNs from scores for averaging, which can occur if a seed was skipped\n",
    "all_val_f1_scores_cleaned = [s for s in all_val_f1_scores if not np.isnan(s)]\n",
    "all_val_auc_scores_cleaned = [s for s in all_val_auc_scores if not np.isnan(s)]\n",
    "all_test_f1_scores_cleaned = [s for s in all_test_f1_scores if not np.isnan(s)]\n",
    "all_test_auc_scores_cleaned = [s for s in all_test_auc_scores if not np.isnan(s)]\n",
    "valid_external_accuracies = [acc for acc in all_external_cancer_accuracies if not np.isnan(acc)]\n",
    "\n",
    "\n",
    "print(f\"\\n--- Average Validation Results over {len(all_val_f1_scores_cleaned)} valid seeds ---\")\n",
    "if all_val_f1_scores_cleaned:\n",
    "    print(f\"Average F1 Score: {np.mean(all_val_f1_scores_cleaned):.4f} (+/- {np.std(all_val_f1_scores_cleaned):.4f})\")\n",
    "    print(f\"Average AUC: {np.mean(all_val_auc_scores_cleaned):.4f} (+/- {np.std(all_val_auc_scores_cleaned):.4f})\")\n",
    "else:\n",
    "    print(\"No valid validation results to average.\")\n",
    "\n",
    "print(f\"\\n--- Average Test Set Results over {len(all_test_f1_scores_cleaned)} valid seeds ---\")\n",
    "if all_test_f1_scores_cleaned:\n",
    "    print(f\"Average F1 Score: {np.mean(all_test_f1_scores_cleaned):.4f} (+/- {np.std(all_test_f1_scores_cleaned):.4f})\")\n",
    "    print(f\"Average AUC: {np.mean(all_test_auc_scores_cleaned):.4f} (+/- {np.std(all_test_auc_scores_cleaned):.4f})\")\n",
    "else:\n",
    "    print(\"No valid test results to average.\")\n",
    "\n",
    "print(f\"\\nEvaluation complete. Results logged to {log_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
